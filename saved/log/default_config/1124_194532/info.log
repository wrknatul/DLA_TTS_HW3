2023-11-24 19:45:35,748 - train - INFO - FastSpeech(
  (encoder): Encoder(
    (src_word_emb): Embedding(300, 256, padding_idx=0)
    (position_enc): Embedding(3001, 256, padding_idx=0)
    (layer_stack): ModuleList(
      (0-3): 4 x FFTBlock(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Conv1d(256, 1024, kernel_size=(9,), stride=(1,), padding=(4,))
          (w_2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (length_regulator): LengthRegulator(
    (duration_predictor): Predictor(
      (conv_layer): ModuleDict(
        (transpose1): Transpose()
        (conv1d_1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        (transpose2): Transpose()
        (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (relu_1): ReLU()
        (dropout_1): Dropout(p=0.1, inplace=False)
        (transpose3): Transpose()
        (conv1d_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        (transpose4): Transpose()
        (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (relu_2): ReLU()
        (dropout_2): Dropout(p=0.1, inplace=False)
      )
      (linear_layer): Linear(in_features=256, out_features=1, bias=True)
      (relu): ReLU()
    )
  )
  (energy_predictor): Predictor(
    (conv_layer): ModuleDict(
      (transpose1): Transpose()
      (conv1d_1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      (transpose2): Transpose()
      (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (relu_1): ReLU()
      (dropout_1): Dropout(p=0.1, inplace=False)
      (transpose3): Transpose()
      (conv1d_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      (transpose4): Transpose()
      (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (relu_2): ReLU()
      (dropout_2): Dropout(p=0.1, inplace=False)
    )
    (linear_layer): Linear(in_features=256, out_features=1, bias=True)
    (relu): ReLU()
  )
  (energy_embedding): Embedding(256, 256)
  (pitch_predictor): Predictor(
    (conv_layer): ModuleDict(
      (transpose1): Transpose()
      (conv1d_1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      (transpose2): Transpose()
      (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (relu_1): ReLU()
      (dropout_1): Dropout(p=0.1, inplace=False)
      (transpose3): Transpose()
      (conv1d_2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      (transpose4): Transpose()
      (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (relu_2): ReLU()
      (dropout_2): Dropout(p=0.1, inplace=False)
    )
    (linear_layer): Linear(in_features=256, out_features=1, bias=True)
    (relu): ReLU()
  )
  (pitch_embedding): Embedding(256, 256)
  (decoder): Decoder(
    (position_enc): Embedding(3001, 256, padding_idx=0)
    (layer_stack): ModuleList(
      (0-3): 4 x FFTBlock(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Conv1d(256, 1024, kernel_size=(9,), stride=(1,), padding=(4,))
          (w_2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mel_linear): Linear(in_features=256, out_features=80, bias=True)
)
